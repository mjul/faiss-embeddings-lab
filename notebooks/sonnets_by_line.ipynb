{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pathlib\n",
    "import urllib.request\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL_DIR = pathlib.Path().absolute().parent / \"models\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the device to use, using a CUDA GPU if available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "model_name = ['bert-base-uncased', 'bert-large-uncased'][1]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=MODEL_DIR)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download the sonnets (free for non-commercial use)\n",
    "url = \"https://flgr.sh/txtfssSontxt\"\n",
    "document = [b.decode('UTF-8') for b in urllib.request.urlopen(url).readlines()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "without_header = list(itertools.dropwhile(lambda x: len(x.strip()) > 0, document))\n",
    "cleaned = [str(line).strip() for line in without_header]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sonnet_number = None\n",
    "sonnets = {}\n",
    "in_between_sonnets = True\n",
    "\n",
    "for line in cleaned:\n",
    "    is_empty = len(line) == 0\n",
    "    if in_between_sonnets:\n",
    "        if is_empty:\n",
    "            pass\n",
    "        elif line.isnumeric():\n",
    "            sonnet_number = int(line)\n",
    "            sonnets[sonnet_number] = []\n",
    "        elif sonnet_number is not None:\n",
    "            in_between_sonnets = False\n",
    "            sonnets[sonnet_number].append(line)\n",
    "        else:\n",
    "            # wait for sonnet number\n",
    "            pass\n",
    "    else:\n",
    "        if is_empty:\n",
    "            in_between_sonnets = True\n",
    "            sonnet_number = None\n",
    "        else:\n",
    "            sonnets[sonnet_number].append(line)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def canonicalize(s):\n",
    "    no_punctuation = ''.join([c for c in s if c.isalpha() or c == ' '])\n",
    "    return no_punctuation.lower().strip()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def encode(strs):\n",
    "    # The Bert paper mentions prepending a [CLS] token and adding a [SEP] token to separate sentences\n",
    "    # https://arxiv.org/pdf/1810.04805.pdf\n",
    "    # However, this seems to make the scores worse, so we don't do it\n",
    "    with torch.no_grad():\n",
    "        encoded_input = tokenizer(strs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "        model_output = model(**encoded_input)\n",
    "    return model_output.last_hidden_state[:, 0, :].detach().cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame([{'sonnet_number': sonnet_number, 'line_number': line_index+1, 'text': text,\n",
    "                    'embeddings': encode([canonicalize(text)])[0]}\n",
    "                   for sonnet_number, lines in sonnets.items()\n",
    "                   for line_index, text in enumerate(lines)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings = np.vstack(df.embeddings.values)\n",
    "print(embeddings.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    xq = encode([canonicalize(query)])\n",
    "    D, I = index.search(xq, k=10)\n",
    "    result = df.iloc[I[0]][['sonnet_number', 'line_number', 'text']]\n",
    "    result['distance'] = D[0]\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search(\"rough winds shake the flowers of spring\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the most similar lines\n",
    "search(\"Rough winds do shake the darling buds of May,\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
